import argparse
import json
from pathlib import Path
from typing import Dict, Optional

import torch

from delr import build_dcelr
from delr.datasets import AarizCephalometricDataset
from delr.metrics import compute_mre_and_sdr, DEFAULT_THRESHOLDS_MM


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Inference script for DeLR (Dual-encoder Landmark Regression).")
    parser.add_argument(
        "--dataset-root",
        type=Path,
        default=Path("/teamspace/studios/this_studio/Aariz/Aariz"),
        help="Root directory of the Aariz dataset.",
    )
    parser.add_argument("--split", type=str, default="test", choices=["train", "valid", "test"])
    parser.add_argument("--checkpoint", type=Path, required=True, help="Path to trained checkpoint (.pt).")
    parser.add_argument("--batch-size", type=int, default=2)
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--output", type=Path, default=Path("./predictions.json"))
    parser.add_argument("--max-samples", type=int, default=None, help="Limit samples for quick smoke tests.")
    parser.add_argument(
        "--backbone",
        type=str,
        default="convnextv2_base",
        help="Backbone to instantiate the model with (e.g., convnextv2_base, convnextv2_tiny, resnet34).",
    )
    parser.add_argument(
        "--skip-metrics",
        action="store_true",
        help="Skip metric computation even if ground-truth annotations are available.",
    )
    return parser.parse_args()


def append_predictions(
    predictions: Dict[str, list],
    batch_coords: torch.Tensor,
    metas: Dict[str, torch.Tensor],
) -> None:
    for idx, coords_pred in enumerate(batch_coords):
        sx = metas["scale_x"][idx]
        sy = metas["scale_y"][idx]
        if isinstance(sx, torch.Tensor):
            sx = float(sx.item())
        if isinstance(sy, torch.Tensor):
            sy = float(sy.item())

        coords_orig = coords_pred.clone()
        coords_orig[:, 0] /= sx
        coords_orig[:, 1] /= sy
        image_id = metas["image_id"][idx]
        if isinstance(image_id, torch.Tensor):
            image_id = image_id.item()
        predictions[str(image_id)] = coords_orig.tolist()


def main() -> None:
    args = parse_args()
    device = torch.device(args.device)

    dataset = AarizCephalometricDataset(
        dataset_root=args.dataset_root,
        split=args.split,
        return_heatmap=False,
    )
    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
    )

    checkpoint = torch.load(args.checkpoint, map_location=device)
    model = build_dcelr(
        num_landmarks=dataset.num_landmarks,
        in_channels=1,
        backbone=args.backbone,
    ).to(device)
    state_dict = checkpoint.get("state_dict", checkpoint)
    model.load_state_dict(state_dict)
    model.eval()

    predictions: Dict[str, list] = {}
    sum_mre_mm = 0.0
    sum_mre_px = 0.0
    total_points = 0.0
    threshold_keys = [str(thr).replace(".", "_") for thr in DEFAULT_THRESHOLDS_MM]
    sdr_hits = {key: 0.0 for key in threshold_keys}

    processed = 0
    for batch in loader:
        if len(batch) == 3:
            images, coords, metas = batch
        else:
            raise ValueError("Unexpected batch format from dataset.")
        images = images.to(device, non_blocking=True)

        out = model(images)
        preds = out["fine_mu"].cpu()
        append_predictions(predictions, preds, metas)

        if not args.skip_metrics:
            metrics = compute_mre_and_sdr(preds, coords, metas, DEFAULT_THRESHOLDS_MM)
            sum_mre_mm += metrics["sum_mre_mm"]
            sum_mre_px += metrics["sum_mre_px"]
            total_points += metrics["num_points"]
            for key in threshold_keys:
                sdr_hits[key] += metrics[f"sdr_hits_{key}mm"]

        processed += len(images)
        if args.max_samples is not None and processed >= args.max_samples:
            break

    args.output.parent.mkdir(parents=True, exist_ok=True)
    with args.output.open("w") as f:
        json.dump(predictions, f, indent=2)

    print(f"Saved predictions to {args.output}")

    if not args.skip_metrics and total_points > 0:
        mre_mm = sum_mre_mm / total_points
        mre_px = sum_mre_px / total_points
        print(f"MRE(mm): {mre_mm:.3f} | MRE(px): {mre_px:.3f}")
        sdr_line = " / ".join(
            f"{(sdr_hits[key] / total_points) * 100.0:.2f}%@{key.replace('_', '.')}mm" for key in threshold_keys
        )
        print(f"SDR: {sdr_line}")


if __name__ == "__main__":
    main()
